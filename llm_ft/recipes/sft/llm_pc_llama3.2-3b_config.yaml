# ModelConfig
model_name_or_path: meta-llama/Llama-3.2-3B-Instruct
model_revision: main
torch_dtype: bfloat16
trust_remote_code: true

# ScriptArguments
dataset_name: data_prepare/datas/llm_pc_llama

# SFTConfig
bf16: true
gradient_checkpointing: true
per_device_train_batch_size: 2
max_seq_length: 2048
eval_strategy: 'no'
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
num_train_epochs: 5.0
output_dir: outputs/llama3.2-3b-llm_pc
save_only_model: true
dataset_kwargs:
  add_special_tokens: false